{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is logged in outputs/firewalls/gradient-matching/sunglasses/RESNET50/7-3_sunglasses_0.1_16_signAdam_250.txt\n",
      "Tuesday, 28. January 2025 04:47PM\n",
      "------------------ Currently evaluating gradient-matching ------------------\n",
      "Namespace(f='/home/thinh.dd/.local/share/jupyter/runtime/kernel-v3c0ce7d63ec54439628803d6b35f22ec2030cfe3e.json', net=['ResNet50'], dataset='Facial_recognition', recipe='gradient-matching', threatmodel='clean-single-source', num_source_classes=1, scenario='finetuning', poisonkey='7-3', system_seed=None, poison_seed=123456, model_seed=123456, deterministic=False, name='', poison_path='poisons/', model_savepath='models/', mixing_method=None, mixing_disable_correction=True, mixing_strength=None, disable_adaptive_attack=True, defend_features_only=False, gradient_noise=None, gradient_clip=None, defense_type=None, defense_strength=None, defense_steps=None, defense_sources=None, padversarial=None, pmix=False, attackoptim='signAdam', attackiter=250, init='randn', tau=0.1, scheduling=True, poison_scheduler='cosine', source_criterion='cross-entropy', restarts=1, pbatch=64, paugment=True, data_aug='default', full_data=False, ensemble=1, stagger=False, step=False, train_max_epoch=40, ablation=1.0, loss='similarity', repel_loss=False, centreg=0, normreg=0, repel=0, visreg=None, vis_weight=1, featreg=0.0, scale=1.0, nadapt=1, clean_grad=False, vruns=3, vnet=None, retrain_from_init=False, skip_clean_training=False, optimization='conservative-sgd', batch_size=64, lr=0.1, noaugment=False, cache_dataset=False, dryrun=False, save_poison=None, save_clean_model=False, save_backdoored_model=False, exp_name='firewalls', local_rank=None, keep_sources=True, sources_train_rate=0.75, sources_selection_rate=1.0, source_gradient_batch=64, val_max_epoch=20, retrain_max_epoch=20, retrain_scenario=None, load_feature_repr=True, train_from_scratch=False, trigger='sunglasses', digital_train=False, digital_test=False, digital_trigger_path='digital_triggers', opacity=0.12549019607843137, retrain_iter=100, source_selection_strategy=None, poison_selection_strategy='max_gradient', eps=16, alpha=0.1, defense=None, firewall=None, inspection_path=None, clean_budget=0.2, subset_size=0.1, subset_freq=2, subset_sampler='coreset-pred', drop_after=1, stop_after=40, equal_num=False, cluster_thresh=1.0, greedy='LazyGreedy', metric='euclidean', checkpoints='../../checkpoints/', devices='0,1', result='./results', defense_set='testset', attack_mode='all2one', nc_scenario='random-poison', temps='./temps', nc_lr=0.1, input_height=224, input_width=224, input_channel=3, init_cost=0.001, atk_succ_threshold=99.0, early_stop=True, early_stop_threshold=99.0, early_stop_patience=10, patience=5, cost_multiplier=2, nc_epoch=60, lr_decay_step=5, lr_decay_factor=0.95, target_label=1, total_label=8, EPSILON=1e-07, to_file=True, n_times_test=1, random_placement=False, output='outputs/firewalls/gradient-matching/sunglasses/RESNET50/7-3_sunglasses_0.1_16_signAdam_250.txt')\n",
      "CPUs: 32, GPUs: 2 on vishc-server-1.\n",
      "GPU : NVIDIA GeForce RTX 3090\n",
      "ResNet50 model initialized with random key 123456.\n",
      "Hyperparameters(name='conservative', epochs=40, batch_size=64, optimizer='SGD', lr=0.001, scheduler='linear', weight_decay=0.0005, augmentations='default', privacy={'clip': None, 'noise': None, 'distribution': None}, validate=10, novel_defense=None, mixing_method=None, adaptive_attack=True, defend_features_only=False)\n",
      "Normalization disabled.\n",
      "Initializing poison data with random seed 123456\n",
      "Setup triggerset\n",
      "Get class distribution of trainset\n",
      "Get class distribution of suspicionset\n",
      "Starting clean training with finetuning scenario ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thinh.dd/Clean-Label-Physical-Backdoor-Attacks/forest/victims/victim_single.py:288: UserWarning: Batch size changed to 40 to fit source train size\n",
      "  warnings.warn(f'Batch size changed to {batch_size} to fit source train size')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50 model initialized with random key 1867215825.\n",
      "Hyperparameters(name='conservative', epochs=40, batch_size=64, optimizer='SGD', lr=0.001, scheduler='linear', weight_decay=0.0005, augmentations='default', privacy={'clip': None, 'noise': None, 'distribution': None}, validate=10, novel_defense=None, mixing_method=None, adaptive_attack=True, defend_features_only=False)\n",
      "ResNet50 model initialized with random key 115388212.\n",
      "Hyperparameters(name='conservative', epochs=40, batch_size=64, optimizer='SGD', lr=0.001, scheduler='linear', weight_decay=0.0005, augmentations='default', privacy={'clip': None, 'noise': None, 'distribution': None}, validate=10, novel_defense=None, mixing_method=None, adaptive_attack=True, defend_features_only=False)\n",
      "ResNet50 model initialized with random key 72108640.\n",
      "Hyperparameters(name='conservative', epochs=40, batch_size=64, optimizer='SGD', lr=0.001, scheduler='linear', weight_decay=0.0005, augmentations='default', privacy={'clip': None, 'noise': None, 'distribution': None}, validate=10, novel_defense=None, mixing_method=None, adaptive_attack=True, defend_features_only=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import forest\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from forest.utils import write, set_random_seed\n",
    "from forest.consts import BENCHMARK, SHARING_STRATEGY\n",
    "\n",
    "torch.backends.cudnn.benchmark = BENCHMARK\n",
    "torch.multiprocessing.set_sharing_strategy(SHARING_STRATEGY)\n",
    "\n",
    "# Parse input arguments\n",
    "args = forest.options().parse_args()\n",
    "args = forest.options().parse_args()\n",
    "args.poisonkey = '7-3'\n",
    "args.trigger = 'sunglasses'\n",
    "args.alpha = 0.1\n",
    "args.eps = 16\n",
    "args.recipe = 'gradient-matching'\n",
    "args.poison_seed = 123456\n",
    "args.model_seed = 123456\n",
    "args.val_max_epoch = 20\n",
    "args.vruns = 1\n",
    "args.exp_name = 'firewalls'\n",
    "args.poison_selection_strategy = 'max_gradient'\n",
    "\n",
    "if args.recipe == 'naive' or args.recipe == 'label-consistent': \n",
    "    args.threatmodel = 'clean-multi-source'\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.devices\n",
    "\n",
    "if args.system_seed != None:\n",
    "    set_random_seed(args.system_seed)\n",
    "\n",
    "if args.exp_name is None:\n",
    "    exp_num = len(os.listdir(os.path.join(os.getcwd(), 'outputs'))) + 1\n",
    "    args.exp_name = f'exp_{exp_num}'\n",
    "\n",
    "args.output = f'outputs/{args.exp_name}/{args.recipe}/{args.trigger}/{args.net[0].upper()}/{args.poisonkey}_{args.trigger}_{args.alpha}_{args.eps}_{args.attackoptim}_{args.attackiter}.txt'\n",
    "print(\"Output is logged in\", args.output)\n",
    "os.makedirs(os.path.dirname(args.output), exist_ok=True)\n",
    "open(args.output, 'w').close() # Clear the output files\n",
    "\n",
    "if args.deterministic:\n",
    "    forest.utils.set_deterministic()\n",
    "\n",
    "\n",
    "setup = forest.utils.system_startup(args) # Set up device and torch data type\n",
    "\n",
    "num_classes = len(os.listdir(os.path.join(\"datasets\",args.dataset, 'train')))\n",
    "model = forest.Victim(args, num_classes=num_classes, setup=setup) # Initialize model and loss_fn\n",
    "data = forest.Kettle(args, model.defs.batch_size, model.defs.augmentations,\n",
    "                    model.defs.mixing_method, setup=setup) # Set up trainloader, validloader, poisonloader, poison_ids, trainset/poisonset/source_testset\n",
    "witch = forest.Witch(args, setup=setup)\n",
    "\n",
    "if args.skip_clean_training:\n",
    "    write('Skipping clean training...', args.output)\n",
    "else:\n",
    "    model.train(data, max_epoch=args.train_max_epoch)\n",
    "            \n",
    "# Select poisons based on maximum gradient norm\n",
    "data.select_poisons(model)\n",
    "\n",
    "# Print data status\n",
    "data.print_status()\n",
    "    \n",
    "poison_delta = witch.brew(model, data)\n",
    "model.validate(data, poison_delta, val_max_epoch=args.val_max_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.59it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy: 0.99%, ASR: 1.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.99375, 1.0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def eval_model(model, kettle):\n",
    "    model.eval()\n",
    "\n",
    "    clean_acc, asr = 0, 0\n",
    "    corrects = 0\n",
    "    for batch_idx, (data, target, idxs) in enumerate(tqdm(kettle.validloader)):\n",
    "        data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        corrects += torch.eq(pred, target).sum().item()\n",
    "    clean_acc = corrects / len(kettle.validloader.dataset)\n",
    "\n",
    "    source_class = kettle.poison_setup['source_class'][0]\n",
    "    target_class = kettle.poison_setup['target_class']\n",
    "\n",
    "    corrects = 0\n",
    "    for batch_idx, (data, _, _) in enumerate(tqdm(kettle.source_testloader[source_class])):\n",
    "        data = data.cuda()\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        corrects += torch.eq(pred, target_class).sum().item()\n",
    "    asr = corrects / len(kettle.source_testloader[source_class].dataset)\n",
    "\n",
    "    print(f\"Clean Accuracy: {clean_acc:.2f}%, ASR: {asr:.2f}%\")\n",
    "    return clean_acc, asr\n",
    "\n",
    "\n",
    "eval_model(model.model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.validloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import copy\n",
    "import albumentations\n",
    "import math\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import torchvision.transforms.v2 as transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from forest.data.datasets import ImageDataset\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Subset\n",
    "from sklearn import metrics\n",
    "from scipy.fftpack import dct, idct\n",
    "from forest.consts import NON_BLOCKING\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch import nn, optim\n",
    "\n",
    "class Strip():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.model.cuda()\n",
    "\n",
    "    def scan(self, kettle, defense_fpr=0.1, batch_size=64):\n",
    "        # choose a decision boundary with the test set\n",
    "        clean_entropy = []\n",
    "        clean_set_loader = torch.utils.data.DataLoader(kettle.validset, batch_size=batch_size, shuffle=False)\n",
    "        for _input, _label, _ in tqdm(clean_set_loader):\n",
    "            _input, _label = _input.to(**kettle.setup), _label.to(dtype=torch.long, device=kettle.setup['device'], non_blocking=NON_BLOCKING)\n",
    "            entropies = self.check(_input, _label, kettle.validset, self.model)\n",
    "            for e in entropies:\n",
    "                clean_entropy.append(e)\n",
    "        clean_entropy = torch.FloatTensor(clean_entropy)\n",
    "\n",
    "        clean_entropy, _ = clean_entropy.sort()\n",
    "        \n",
    "        threshold_low = float(clean_entropy[int(defense_fpr * len(clean_entropy))])\n",
    "\n",
    "        # Calculate TPR\n",
    "        all_entropy = []\n",
    "        source_class = kettle.poison_setup['source_class'][0]\n",
    "        for _input, _label, _ in tqdm(kettle.source_testloader[source_class]):\n",
    "            _input, _label = _input.to(**kettle.setup), _label.to(dtype=torch.long, device=kettle.setup['device'], non_blocking=NON_BLOCKING)\n",
    "            entropies = self.check(_input, _label, kettle.validset, self.model)\n",
    "            for e in entropies:\n",
    "                all_entropy.append(e)\n",
    "        all_entropy = torch.FloatTensor(all_entropy)\n",
    "        true_positives = (all_entropy < threshold_low).sum().item()\n",
    "        true_positive_rate = true_positives / len(kettle.source_testloader[source_class].dataset)\n",
    "\n",
    "        # Calculate FPR\n",
    "        all_entropy = []\n",
    "        for _input, _label, _ in tqdm(kettle.validloader):\n",
    "            _input, _label = _input.to(**kettle.setup), _label.to(dtype=torch.long, device=kettle.setup['device'], non_blocking=NON_BLOCKING)\n",
    "            entropies = self.check(_input, _label, kettle.validset, self.model)\n",
    "            for e in entropies:\n",
    "                all_entropy.append(e)\n",
    "        all_entropy = torch.FloatTensor(all_entropy)\n",
    "        false_positives = (all_entropy < threshold_low).sum().item()\n",
    "        false_positive_rate = false_positives / len(kettle.validloader.dataset)\n",
    "\n",
    "        print(f\"True Positive Rate (TPR): {true_positive_rate:.4f}\")\n",
    "        print(f\"False Positive Rate (FPR): {false_positive_rate:.4f}\")\n",
    "        \n",
    "        return true_positive_rate, false_positive_rate\n",
    "    \n",
    "    def check(self, _input, _label, source_set, model, N=200):\n",
    "        _list = []\n",
    "\n",
    "        samples = list(range(len(source_set)))\n",
    "        random.shuffle(samples)\n",
    "        samples = samples[:N]\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for i in samples:\n",
    "                X, _, _ = source_set[i]\n",
    "                X = X.cuda()\n",
    "                _test = self.superimpose(_input, X)\n",
    "                entro = self.entropy(_test, model).cpu().detach()\n",
    "                _list.append(entro)\n",
    "\n",
    "        return torch.stack(_list).mean(0)\n",
    "    \n",
    "    def superimpose(self, _input1, _input2, alpha = None):\n",
    "        if alpha is None:\n",
    "            alpha = 0.5\n",
    "\n",
    "        result = _input1 + alpha * _input2\n",
    "        return result\n",
    "\n",
    "    def entropy(self, _input, model) -> torch.Tensor:\n",
    "        p = torch.nn.Softmax(dim=1)(model(_input)) + 1e-8\n",
    "        return (-p * p.log()).sum(1)\n",
    "\n",
    "class IBD_PSC():\n",
    "    \"\"\"Identify and filter malicious testing samples (IBD-PSC).\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The original backdoored model.\n",
    "        n (int): The hyper-parameter for the number of parameter-amplified versions of the original backdoored model by scaling up of its different BN layers.\n",
    "        xi (float): The hyper-parameter for the error rate.\n",
    "        T (float):  The hyper-parameter for defender-specified threshold T. If PSC(x) > T , we deem it as a backdoor sample.\n",
    "        scale (float): The hyper-parameter for amplyfying the parameters of selected BN layers.\n",
    "        seed (int): Global seed for random numbers. Default: 0.\n",
    "        deterministic (bool): Sets whether PyTorch operations must use \"deterministic\" algorithms.\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, model, n=5, xi=0.6, T = 0.9, scale=1.5, valset=None, seed=666, deterministic=False):\n",
    "        self.model = model\n",
    "        self.model.cuda()\n",
    "        self.model.eval()\n",
    "        self.n = n\n",
    "        self.xi = xi\n",
    "        self.T = T\n",
    "        self.scale = scale\n",
    "        self.valset = valset\n",
    "\n",
    "        layer_num = self.count_BN_layers()\n",
    "        sorted_indices = list(range(layer_num))\n",
    "        sorted_indices = list(reversed(sorted_indices))\n",
    "        self.sorted_indices = sorted_indices\n",
    "        self.start_index = self.prob_start(self.scale, self.sorted_indices, valset=self.valset)\n",
    "\n",
    "    def count_BN_layers(self):\n",
    "        layer_num = 0\n",
    "        for (name1, module1) in self.model.named_modules():\n",
    "            if isinstance(module1, torch.nn.BatchNorm2d):\n",
    "                layer_num += 1\n",
    "        return layer_num\n",
    "\n",
    "    def scale_var_index(self, index_bn, scale=1.5):\n",
    "        copy_model = copy.deepcopy(self.model)\n",
    "        index  = -1\n",
    "        for (name1, module1) in copy_model.named_modules():\n",
    "            if isinstance(module1, torch.nn.BatchNorm2d):\n",
    "                index += 1\n",
    "                if index in index_bn:\n",
    "                    module1.weight.data *= scale\n",
    "                    module1.bias.data *= scale\n",
    "        return copy_model  \n",
    "    \n",
    "    def prob_start(self, scale, sorted_indices, valset):\n",
    "        val_loader = torch.utils.data.DataLoader(valset, batch_size=128, shuffle=False)\n",
    "        layer_num = len(sorted_indices)\n",
    "        # layer_index: k\n",
    "        for layer_index in range(1, layer_num):            \n",
    "            layers = sorted_indices[:layer_index]\n",
    "            # print(layers)\n",
    "            smodel = self.scale_var_index(layers, scale=scale)\n",
    "            smodel.cuda()\n",
    "            smodel.eval()\n",
    "            \n",
    "            total_num = 0 \n",
    "            clean_wrong = 0\n",
    "            with torch.no_grad():\n",
    "                for idx, batch in enumerate(val_loader):\n",
    "                    clean_img = batch[0]\n",
    "                    labels = batch[1]\n",
    "                    clean_img = clean_img.cuda()  # batch * channels * hight * width\n",
    "                    # labels = labels.cuda()  # batch\n",
    "                    clean_logits = smodel(clean_img).detach().cpu()\n",
    "                    clean_pred = torch.argmax(clean_logits, dim=1)# model prediction\n",
    "                    \n",
    "                    clean_wrong += torch.sum(labels != clean_pred)\n",
    "                    total_num += labels.shape[0]\n",
    "                wrong_acc = clean_wrong / total_num\n",
    "                # print(f'wrong_acc: {wrong_acc}')\n",
    "                if wrong_acc > self.xi:\n",
    "                    return layer_index\n",
    "\n",
    "    def _test(self, dataset):\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "        self.model.eval()\n",
    "        total_num = 0\n",
    "        all_psc_score = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(data_loader):\n",
    "                imgs = batch[0]\n",
    "                labels = batch[1]\n",
    "                total_num += labels.shape[0]\n",
    "                imgs = imgs.cuda()  # batch * channels * hight * width\n",
    "                labels = labels.cuda()  # batch\n",
    "                original_pred = torch.argmax(self.model(imgs), dim=1) # model prediction\n",
    "\n",
    "                psc_score = torch.zeros(labels.shape)\n",
    "                scale_count = 0\n",
    "                for layer_index in range(self.start_index, self.start_index + self.n):\n",
    "                    layers = self.sorted_indices[:layer_index+1]\n",
    "                    # print(f'layers: {layers}')\n",
    "                    smodel = self.scale_var_index(layers, scale=self.scale)\n",
    "                    scale_count += 1\n",
    "                    smodel.eval()\n",
    "                    logits = smodel(imgs).detach().cpu()\n",
    "                    softmax_logits = torch.nn.functional.softmax(logits, dim=1)\n",
    "                    psc_score += softmax_logits[torch.arange(softmax_logits.size(0)), original_pred.cpu()]\n",
    "\n",
    "                psc_score /= scale_count\n",
    "                all_psc_score.append(psc_score)\n",
    "        \n",
    "        all_psc_score = torch.cat(all_psc_score, dim=0)\n",
    "        return all_psc_score\n",
    "    \n",
    "    def scan(self, kettle):\n",
    "        print(f'start_index: {self.start_index}')\n",
    "\n",
    "        testset = kettle.validset\n",
    "        source_class = kettle.poison_setup['source_class'][0]\n",
    "        poisoned_testset = kettle.source_testset[source_class]\n",
    "        print(len(poisoned_testset))\n",
    "        \n",
    "        benign_psc = self._test(testset)\n",
    "        poison_psc = self._test(poisoned_testset)\n",
    "\n",
    "        true_positive_rate = (poison_psc >= self.T).sum().item() / len(poison_psc)\n",
    "        false_positive_rate = (benign_psc >= self.T).sum().item() / len(benign_psc)\n",
    "\n",
    "        print(\"True Positive Rate: {:.4f}\".format(true_positive_rate))\n",
    "        print(\"False Positive Rate: {:.4f}\".format(false_positive_rate))\n",
    "\n",
    "    def _detect(self, inputs):\n",
    "        inputs = inputs.cuda()\n",
    "        self.model.eval()\n",
    "        self.model.cuda()\n",
    "        original_pred = torch.argmax(self.model(inputs), dim=1) # model prediction\n",
    "\n",
    "        psc_score = torch.zeros(inputs.size(0))\n",
    "        scale_count = 0\n",
    "        for layer_index in range(self.start_index, self.start_index + self.n):\n",
    "            layers = self.sorted_indices[:layer_index+1]\n",
    "            # print(f'layers: {layers}')\n",
    "            smodel = self.scale_var_index(layers, scale=self.scale)\n",
    "            scale_count += 1\n",
    "            smodel.eval()\n",
    "            logits = smodel(inputs).detach().cpu()\n",
    "            softmax_logits = torch.nn.functional.softmax(logits, dim=1)\n",
    "            psc_score += softmax_logits[torch.arange(softmax_logits.size(0)), original_pred]\n",
    "\n",
    "        psc_score /= scale_count\n",
    "        \n",
    "        y_pred = psc_score >= self.T\n",
    "        return y_pred\n",
    "    \n",
    "    def detect(self, dataset):\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "        with torch.no_grad():\n",
    "            for idx, batch in enumerate(data_loader):\n",
    "                imgs = batch[0]\n",
    "                return self._detect(imgs)\n",
    "\n",
    "def total_variation_loss(img, weight=1):\n",
    "    b, c, h, w = img.size()\n",
    "    tv_h = torch.pow(img[:, :, 1:, :]-img[:, :, :-1, :], 2).sum(dim=[1, 2, 3])\n",
    "    tv_w = torch.pow(img[:, :, :, 1:]-img[:, :, :, :-1], 2).sum(dim=[1, 2, 3])\n",
    "    return weight*(tv_h+tv_w)/(c*h*w)\n",
    "\n",
    "\n",
    "class CognitiveDistillation(nn.Module):\n",
    "    def __init__(self, lr=0.1, p=1, gamma=0.01, beta=1.0, num_steps=100, mask_channel=1, norm_only=False):\n",
    "        super(CognitiveDistillation, self).__init__()\n",
    "        self.p = p\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.num_steps = num_steps\n",
    "        self.l1 = torch.nn.L1Loss(reduction='none')\n",
    "        self.lr = lr\n",
    "        self.mask_channel = mask_channel\n",
    "        self.get_features = False\n",
    "        self._EPSILON = 1.e-6\n",
    "        self.norm_only = norm_only\n",
    "\n",
    "    def get_raw_mask(self, mask):\n",
    "        mask = (torch.tanh(mask) + 1) / 2\n",
    "        return mask\n",
    "\n",
    "    def forward(self, model, images, labels=None):\n",
    "        model.eval()\n",
    "        b, c, h, w = images.shape\n",
    "        mask = torch.ones(b, self.mask_channel, h, w).to(images.device)\n",
    "        mask_param = nn.Parameter(mask)\n",
    "        optimizerR = torch.optim.Adam([mask_param], lr=self.lr, betas=(0.1, 0.1))\n",
    "        if self.get_features:\n",
    "            features, logits = model(images)\n",
    "        else:\n",
    "            logits = model(images).detach()\n",
    "        for step in range(self.num_steps):\n",
    "            optimizerR.zero_grad()\n",
    "            mask = self.get_raw_mask(mask_param).to(images.device)\n",
    "            x_adv = images * mask + (1-mask) * torch.rand(b, c, 1, 1).to(images.device)\n",
    "            if self.get_features:\n",
    "                adv_fe, adv_logits = model(x_adv)\n",
    "                if len(adv_fe[-2].shape) == 4:\n",
    "                    loss = self.l1(adv_fe[-2], features[-2].detach()).mean(dim=[1, 2, 3])\n",
    "                else:\n",
    "                    loss = self.l1(adv_fe[-2], features[-2].detach()).mean(dim=1)\n",
    "            else:\n",
    "                adv_logits = model(x_adv)\n",
    "                loss = self.l1(adv_logits, logits).mean(dim=1)\n",
    "            norm = torch.norm(mask, p=self.p, dim=[1, 2, 3])\n",
    "            norm = norm * self.gamma\n",
    "            loss_total = loss + norm + self.beta * total_variation_loss(mask)\n",
    "            loss_total.mean().backward()\n",
    "            optimizerR.step()\n",
    "        mask = self.get_raw_mask(mask_param).detach().cpu()\n",
    "        if self.norm_only:\n",
    "            return torch.norm(mask, p=1, dim=[1, 2, 3])\n",
    "        return mask.detach()\n",
    "    \n",
    "\n",
    "class Frequency:\n",
    "    def __init__(self, weight_path=None):\n",
    "        self.model = nn.Sequential(nn.Flatten(),\n",
    "                                              nn.Linear(3*224*224, 2)\n",
    "                                    )\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        \n",
    "        if weight_path is not None:\n",
    "            self.model.load_state_dict(torch.load(weight_path))\n",
    "     \n",
    "    def train(self, num_epochs=50):\n",
    "        # Process dataset to create a dataset with backdoored samples and benign samples in the frequency domain\n",
    "        dataset = ImageDataset('datasets/Facial_recognition_crop_partial/real_beard/train')\n",
    "        tensor_list = []\n",
    "        labels_list = []\n",
    "        totensor = transforms.Compose([\n",
    "            transforms.ToDtype(torch.float32, scale=True),\n",
    "        ])\n",
    "        \n",
    "        for img, _, _ in dataset:\n",
    "            # 0 means benign, 1 means poisoned\n",
    "            tensor_list.append(totensor(self.dct2(img)))\n",
    "            labels_list.append(0)\n",
    "            \n",
    "            transformed_img = self.patching_train(img)\n",
    "            tensor_list.append(totensor(self.dct2(transformed_img)))\n",
    "            labels_list.append(1)\n",
    "            \n",
    "        labels = torch.tensor(labels_list, dtype=torch.long)\n",
    "        img_tensors = torch.stack(tensor_list)\n",
    "        \n",
    "        train_dataset = torch.utils.data.TensorDataset(img_tensors, labels)\n",
    "        train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        \n",
    "        # Move the model to the GPU\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adadelta(lr=0.05, params=self.model.parameters())\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            correct_predictions = 0\n",
    "            total_predictions = 0\n",
    "\n",
    "            for imgs, labels in train_loader:\n",
    "                # Move data and labels to the GPU\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Accumulate the loss\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                # Calculate the number of correct predictions\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_predictions += (predicted == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "\n",
    "            # Calculate average loss and accuracy for the epoch\n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            epoch_accuracy = correct_predictions / total_predictions\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "        print('Finished Training')\n",
    "        torch.save(self.model.state_dict(), os.path.join('forest/frequency_detect_model', f\"{num_epochs}_epochs.pth\"))\n",
    "\n",
    "    def evaluate(self, attack_list = None):\n",
    "        self.model.eval()\n",
    "        if attack_list == None:\n",
    "            attack_list = ['l0_inv', 'badnets', 'smooth', 'blend', ]\n",
    "\n",
    "        test_set = ImageDataset('datasets/Facial_recognition_crop_partial/real_beard/test')\n",
    "        totensor = transforms.Compose([\n",
    "            transforms.ToImage(), \n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "        ])\n",
    "\n",
    "        for attack in attack_list:\n",
    "            print(f\"Evaluating detector on {attack.capitalize()}\")\n",
    "\n",
    "            clean_samples = []\n",
    "            poison_samples = []\n",
    "\n",
    "            for img, _, _ in test_set:\n",
    "                # 0 means benign, 1 means poisoned\n",
    "                clean_samples.append(totensor(self.dct2(img)))\n",
    "                \n",
    "                transformed_img = self.patching_test(img, attack)\n",
    "                poison_samples.append(totensor(self.dct2(transformed_img)))\n",
    "\n",
    "            clean_samples = torch.stack(clean_samples)\n",
    "            clean_labels = torch.zeros(clean_samples.size(0), dtype=torch.long)\n",
    "\n",
    "            poison_samples = torch.stack(poison_samples)\n",
    "            poison_labels = torch.ones(poison_samples.size(0), dtype=torch.long)\n",
    "\n",
    "            clean_testset = torch.utils.data.TensorDataset(clean_samples, clean_labels)\n",
    "            poison_testset = torch.utils.data.TensorDataset(poison_samples, poison_labels)\n",
    "\n",
    "            clean_testloader = torch.utils.data.DataLoader(clean_testset, batch_size=32, shuffle=False)\n",
    "            poison_testloader = torch.utils.data.DataLoader(poison_testset, batch_size=32, shuffle=False)\n",
    "\n",
    "            return self.scan(clean_testloader, poison_testloader)\n",
    "\n",
    "    def scan(self, kettle):\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "        true_negatives = 0\n",
    "        false_negatives = 0\n",
    "\n",
    "        clean_set = kettle.validset\n",
    "        clean_set.transform = None\n",
    "\n",
    "        source_class = kettle.poison_setup['source_class'][0]\n",
    "        poison_set = kettle.source_testset[source_class]\n",
    "        poison_set.dataset.transform  = None\n",
    "\n",
    "        totensor = transforms.Compose([\n",
    "            transforms.ToImage(), \n",
    "            transforms.ToDtype(torch.float32, scale=True),\n",
    "        ])\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            # Test on clean samples\n",
    "            for inputs, _, _ in clean_set:\n",
    "                transformed_inputs = totensor(self.dct2(inputs)).unsqueeze(0).cuda()\n",
    "\n",
    "                outputs = self.model(transformed_inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # Assuming 0 = clean and 1 = poisoned\n",
    "                true_negatives += (preds == 0).sum().item()\n",
    "                false_positives += (preds == 1).sum().item()\n",
    "\n",
    "            # Test on poisoned samples\n",
    "            for inputs, _, _ in poison_set:\n",
    "                transformed_inputs = totensor(self.dct2(inputs)).unsqueeze(0).cuda()\n",
    "                \n",
    "                outputs = self.model(transformed_inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # Assuming 0 = clean and 1 = poisoned\n",
    "                true_positives += (preds == 1).sum().item()\n",
    "                false_negatives += (preds == 0).sum().item()\n",
    "\n",
    "        # Calculate True Positive Rate (TPR) and False Positive Rate (FPR)\n",
    "        tpr = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        fpr = false_positives / (false_positives + true_negatives) if (false_positives + true_negatives) > 0 else 0\n",
    "\n",
    "        print(f\"True Positive Rate (TPR): {tpr:.4f}\")\n",
    "        print(f\"False Positive Rate (FPR): {fpr:.4f}\")\n",
    "\n",
    "        return tpr, fpr\n",
    "    \n",
    "    @staticmethod\n",
    "    def dct2 (block):\n",
    "        return dct(dct(block, norm='ortho', axis=0), norm='ortho', axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def idct2(block):\n",
    "        return idct(idct(block, norm='ortho', axis=0), norm='ortho', axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def tensor2img(t):\n",
    "        t_np = t.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "        return t_np\n",
    "    \n",
    "    @staticmethod\n",
    "    def addnoise(img):\n",
    "        aug = albumentations.GaussNoise(p=1,mean=25,var_limit=(10,70))\n",
    "        augmented = aug(image=(img*255).astype(np.uint8))\n",
    "        auged = augmented['image']/255\n",
    "        return auged\n",
    "\n",
    "    @staticmethod\n",
    "    def randshadow(img):\n",
    "        aug = albumentations.RandomShadow(p=1)\n",
    "        test = (img*255).astype(np.uint8)\n",
    "        augmented = aug(image=cv2.resize(test,(224,224)))\n",
    "        auged = augmented['image']/255\n",
    "        return auged\n",
    "    \n",
    "    @staticmethod\n",
    "    def gauss_smooth(image, sig=6):\n",
    "        size_denom = 5.\n",
    "        sigma = sig * size_denom\n",
    "        kernel_size = sigma\n",
    "        mgrid = np.arange(kernel_size, dtype=np.float32)\n",
    "        mean = (kernel_size - 1.) / 2.\n",
    "        mgrid = mgrid - mean\n",
    "        mgrid = mgrid * size_denom\n",
    "        kernel = 1. / (sigma * math.sqrt(2. * math.pi)) * \\\n",
    "                np.exp(-(((mgrid - 0.) / (sigma)) ** 2) * 0.5)\n",
    "        kernel = kernel / np.sum(kernel)\n",
    "\n",
    "        # Reshape to depthwise convolutional weight\n",
    "        kernelx = np.tile(np.reshape(kernel, (1, 1, int(kernel_size), 1)), (3, 1, 1, 1))\n",
    "        kernely = np.tile(np.reshape(kernel, (1, 1, 1, int(kernel_size))), (3, 1, 1, 1))\n",
    "\n",
    "        padd0 = int(kernel_size // 2)\n",
    "        evenorodd = int(1 - kernel_size % 2)\n",
    "\n",
    "        pad = torch.nn.ConstantPad2d((padd0 - evenorodd, padd0, padd0 - evenorodd, padd0), 0.)\n",
    "        in_put = torch.from_numpy(np.expand_dims(np.transpose(image.astype(np.float32), (2, 0, 1)), axis=0))\n",
    "        output = pad(in_put)\n",
    "\n",
    "        weightx = torch.from_numpy(kernelx)\n",
    "        weighty = torch.from_numpy(kernely)\n",
    "        conv = F.conv2d\n",
    "        output = conv(output, weightx, groups=3)\n",
    "        output = conv(output, weighty, groups=3)\n",
    "        output = Frequency.tensor2img(output[0])\n",
    "\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def normalization(data):\n",
    "        _range = np.max(data) - np.min(data)\n",
    "        return (data - np.min(data)) / _range\n",
    "    \n",
    "    def patching_train(self, clean_sample):\n",
    "        '''\n",
    "        This code conducts a patching procedure with random white blocks or random noise block\n",
    "        '''\n",
    "        attack = np.random.randint(0,5)\n",
    "        pat_size_x = np.random.randint(14,56)\n",
    "        pat_size_y = np.random.randint(14,56)\n",
    "        output = self.normalization(np.copy(clean_sample))\n",
    "        if attack == 0:\n",
    "            block = np.ones((pat_size_x,pat_size_y,3))\n",
    "        elif attack == 1:\n",
    "            block = np.random.rand(pat_size_x,pat_size_y,3)\n",
    "        elif attack == 2:\n",
    "            return self.addnoise(output)\n",
    "        elif attack == 3:\n",
    "            return self.randshadow(output)\n",
    "        if attack == 4:\n",
    "            randind = np.random.randint(output.shape[0])\n",
    "            tri = output[randind]\n",
    "            mid = output+0.3*tri\n",
    "            mid[mid>1]=1\n",
    "            return mid\n",
    "            \n",
    "        margin = np.random.randint(0,42)\n",
    "        rand_loc = np.random.randint(0,4)\n",
    "        if rand_loc==0:\n",
    "            output[margin:margin+pat_size_x,margin:margin+pat_size_y,:] = block #upper left\n",
    "        elif rand_loc==1:\n",
    "            output[margin:margin+pat_size_x,224-margin-pat_size_y:224-margin,:] = block\n",
    "        elif rand_loc==2:\n",
    "            output[224-margin-pat_size_x:224-margin,margin:margin+pat_size_y,:] = block\n",
    "        elif rand_loc==3:\n",
    "            output[224-margin-pat_size_x:224-margin,224-margin-pat_size_y:224-margin,:] = block #right bottom\n",
    "\n",
    "        output[output > 1] = 1\n",
    "        return output \n",
    "\n",
    "    def patching_test(self, clean_sample, attack_name):\n",
    "        '''\n",
    "        This code conducts a patching procedure to generate backdoor data.\n",
    "        **Please make sure the input sample's label is different from the target label.\n",
    "        \n",
    "        clean_sample: clean input\n",
    "        attack_name: trigger's file name\n",
    "        '''\n",
    "\n",
    "        output = self.normalization(np.copy(clean_sample))\n",
    "\n",
    "        if attack_name == 'badnets':\n",
    "            pat_size = 28\n",
    "            output[224-1-pat_size:224-1,224-1-pat_size:224-1,:] = 1\n",
    "\n",
    "        else:\n",
    "            if attack_name == 'l0_inv':\n",
    "                trimg = cv2.imread('forest/triggers/'+ attack_name + '.png')\n",
    "                # Resize the trigger image to 224x224\n",
    "                trimg_resized = self.normalization(cv2.resize(trimg, dsize=(224, 224), interpolation=cv2.INTER_NEAREST))\n",
    "                \n",
    "                mask = 1 - np.load('forest/triggers/mask.npy').transpose(1,2,0)\n",
    "                # Resize the mask to 224x224\n",
    "                mask_resized = cv2.resize(mask, dsize=(224, 224), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "                output = output * mask_resized + trimg_resized\n",
    "\n",
    "            elif attack_name == 'smooth':\n",
    "                trimg = np.load('forest/triggers/best_universal.npy')[0]\n",
    "                # Resize the trigger image to 224x224\n",
    "                trimg_resized = self.normalization(cv2.resize(trimg, dsize=(224, 224), interpolation=cv2.INTER_CUBIC))\n",
    "                output = output + trimg_resized\n",
    "\n",
    "            else:\n",
    "                trimg = cv2.imread('forest/triggers/' + attack_name + '.png')\n",
    "                # Resize the trigger image to 224x224\n",
    "                trimg_resized = self.normalization(cv2.resize(trimg, dsize=(224, 224), interpolation=cv2.INTER_CUBIC))\n",
    "                \n",
    "                output = output + trimg_resized\n",
    "        \n",
    "        output = self.normalization(output)\n",
    "        # Ensure the output is within valid pixel range [0, 1]\n",
    "        output[output > 1] = 1\n",
    "        return output\n",
    "\n",
    "def min_max_normalization(x):\n",
    "    x_min = torch.min(x)\n",
    "    x_max = torch.max(x)\n",
    "    norm = (x - x_min) / (x_max - x_min)\n",
    "    return norm\n",
    "\n",
    "class CognitiveDistillationAnalysis():\n",
    "    def __init__(self, od_type='l1_norm', norm_only=False):\n",
    "        self.od_type = od_type\n",
    "        self.norm_only = norm_only\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        return\n",
    "\n",
    "    def train(self, data):\n",
    "        if not self.norm_only:\n",
    "            data = torch.norm(data, dim=[1, 2, 3], p=1)\n",
    "        self.mean = torch.mean(data).item()\n",
    "        self.std = torch.std(data).item()\n",
    "        return\n",
    "\n",
    "    def predict(self, data, t=1):\n",
    "        if not self.norm_only:\n",
    "            data = torch.norm(data, dim=[1, 2, 3], p=1)\n",
    "        p = (self.mean - data) / self.std\n",
    "        p = torch.where((p > t) & (p > 0), 1, 0)\n",
    "        return p.numpy()\n",
    "\n",
    "    def analysis(self, data, is_test=False):\n",
    "        \"\"\"\n",
    "            data (torch.tensor) b,c,h,w\n",
    "            data is the distilled mask or pattern extracted by CognitiveDistillation (torch.tensor)\n",
    "        \"\"\"\n",
    "        if self.norm_only:\n",
    "            if len(data.shape) > 1:\n",
    "                data = torch.norm(data, dim=[1, 2, 3], p=1)\n",
    "            score = data\n",
    "        else:\n",
    "            score = torch.norm(data, dim=[1, 2, 3], p=1)\n",
    "        score = min_max_normalization(score)\n",
    "        return 1 - score.numpy()  # Lower for BD\n",
    "    \n",
    "class CognitiveDefense:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def scan(self, kettle):\n",
    "        cd = CognitiveDistillation(lr=0.1, p=1, gamma=0.01, beta=10.0, num_steps=100)\n",
    "\n",
    "        # Run detections on training set\n",
    "        results = []\n",
    "        for images, labels, _ in tqdm(kettle.validloader):\n",
    "            images, labels = images.to(**kettle.setup), labels.to(dtype=torch.long, device=kettle.setup['device'], non_blocking=NON_BLOCKING)\n",
    "            batch_rs = cd(self.model, images, labels)\n",
    "            results.append(batch_rs.detach().cpu())\n",
    "\n",
    "        train_results = torch.cat(results, dim=0)\n",
    "\n",
    "        # Run detections on backdoor test set\n",
    "        results = []\n",
    "        source_class = kettle.poison_setup['source_class'][0]\n",
    "        for images, labels, _ in tqdm(kettle.source_testloader[source_class]):\n",
    "            images, labels = images.to(**kettle.setup), labels.to(dtype=torch.long, device=kettle.setup['device'], non_blocking=NON_BLOCKING)\n",
    "            batch_rs = cd(self.model, images, labels)\n",
    "            results.append(batch_rs.detach().cpu())\n",
    "\n",
    "        test_results = torch.cat(results, dim=0)\n",
    "\n",
    "        detector = CognitiveDistillationAnalysis()\n",
    "\n",
    "        s = len(train_results) * 0.1\n",
    "        s = int(s)\n",
    "        threshold = 0.5\n",
    "\n",
    "        detector.train(train_results[:s])\n",
    "        y_pred_poison = detector.predict(test_results, t=threshold)\n",
    "        true_positive_rate = y_pred_poison.sum() / len(y_pred_poison)\n",
    "\n",
    "        y_pred_clean = detector.predict(train_results, t=threshold)\n",
    "        false_positive_rate = y_pred_clean.sum() / len(y_pred_clean)\n",
    "\n",
    "        print(f\"True Positive Rate (TPR): {true_positive_rate:.4f}\")\n",
    "        print(f\"False Positive Rate (FPR): {false_positive_rate:.4f}\")\n",
    "\n",
    "        return true_positive_rate, false_positive_rate\n",
    "    \n",
    "class ScaleUp():\n",
    "    name: str = 'scale up'\n",
    "\n",
    "    def __init__(self, model, kettle, defense_ratio=0.2, scale_set=None, threshold=None, with_clean_data=True):\n",
    "        if scale_set is None:\n",
    "            scale_set = [3, 5, 7, 9, 11]\n",
    "        if threshold is None:\n",
    "            self.threshold = 0.5\n",
    "\n",
    "        self.scale_set = scale_set\n",
    "        self.model = model\n",
    "        self.kettle = kettle\n",
    "        self.with_clean_data = with_clean_data\n",
    "\n",
    "        size = int(defense_ratio * len(self.kettle.validset))\n",
    "        random_indices = torch.randperm(len(self.kettle.validset))[:size]\n",
    "        self.clean_set = Subset(self.kettle.validset, indices=random_indices)\n",
    "        self.clean_loader = torch.utils.data.DataLoader(self.clean_set, \n",
    "                                                        batch_size=64, \n",
    "                                                        shuffle=False, \n",
    "                                                        num_workers=4, \n",
    "                                                        pin_memory=True\n",
    "                                                    )\n",
    "\n",
    "        # test set --- clean\n",
    "        # std_test - > 10000 full, val -> 2000 (for detection), test -> 8000 (for accuracy)\n",
    "\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        if self.with_clean_data:\n",
    "            self.init_spc_norm()\n",
    "\n",
    "    def detect(self, use_pseudo_labels=True):\n",
    "\n",
    "        false_positives = 0\n",
    "        clean_pred_correct_mask = []\n",
    "        pred_poison_mask = []\n",
    "\n",
    "        for idx, (clean_img, labels, _) in enumerate(self.kettle.validloader):\n",
    "            clean_img = clean_img.cuda()  # batch * channels * hight * width\n",
    "            labels = labels.cuda()  # batch\n",
    "\n",
    "            correct_pred_mask = torch.eq(labels, torch.argmax(self.model(clean_img), dim=1))\n",
    "            clean_pred_correct_mask.append(correct_pred_mask)\n",
    "\n",
    "            # evaluate the clean data\n",
    "            scaled_imgs = []\n",
    "            scaled_labels = []\n",
    "            for scale in self.scale_set:\n",
    "                scaled_imgs.append(torch.clip(clean_img * scale, min=0.0, max=1.0))\n",
    "            for scale_img in scaled_imgs:\n",
    "                scale_label = torch.argmax(self.model(scale_img), dim=1)\n",
    "                scaled_labels.append(scale_label)\n",
    "            clean_pred = torch.argmax(self.model(clean_img), dim=1) # model prediction\n",
    "            # compute the SPC Value\n",
    "            spc_clean = torch.zeros(labels.shape).cuda()\n",
    "            for scale_label in scaled_labels:\n",
    "                spc_clean += scale_label == clean_pred\n",
    "            spc_clean /= len(self.scale_set)\n",
    "\n",
    "            if self.with_clean_data:\n",
    "                spc_clean = (spc_clean - self.mean) / self.std\n",
    "            \n",
    "            pred_poison_mask.append(spc_clean > self.threshold)\n",
    "            false_positives += (spc_clean > self.threshold).sum().item()\n",
    "\n",
    "        clean_pred_correct_mask = torch.cat(clean_pred_correct_mask, dim=0)\n",
    "        pred_poison_mask = torch.cat(pred_poison_mask, dim=0)\n",
    "\n",
    "        print(\"Clean Accuracy: %d/%d = %.6f\" % (clean_pred_correct_mask[torch.logical_not(pred_poison_mask)].sum(), len(self.kettle.validloader.dataset),\n",
    "                                                clean_pred_correct_mask[torch.logical_not(pred_poison_mask)].sum() / len(self.kettle.validloader.dataset)))\n",
    "\n",
    "        print(f\"False Positive Rate (FPR): {false_positives / len(self.kettle.validloader.dataset):.4f}\")\n",
    "\n",
    "        true_positives = 0\n",
    "        poison_attack_success_mask = []\n",
    "        pred_poison_mask = []\n",
    "        source_class = self.kettle.poison_setup['source_class'][0]\n",
    "        target_class = self.kettle.poison_setup['target_class']\n",
    "\n",
    "        for idx, (trigger_img, labels, _) in enumerate(self.kettle.source_testloader[source_class]):\n",
    "            trigger_img = trigger_img.cuda()  # batch * channels * hight * width\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            preds = torch.argmax(self.model(trigger_img), dim=1)\n",
    "            poison_attack_success_mask.append(torch.eq(preds, target_class))\n",
    "\n",
    "            # evaluate the clean data\n",
    "            scaled_imgs = []\n",
    "            scaled_labels = []\n",
    "            for scale in self.scale_set:\n",
    "                scaled_imgs.append(torch.clip(trigger_img * scale, min=0.0, max=1.0))\n",
    "\n",
    "            for scale_img in scaled_imgs:\n",
    "                scale_label = torch.argmax(self.model(scale_img), dim=1)\n",
    "                scaled_labels.append(scale_label)\n",
    "            poison_pred = torch.argmax(self.model(trigger_img), dim=1) # model prediction\n",
    "            # compute the SPC Value\n",
    "            spc_poison = torch.zeros(labels.shape).cuda()\n",
    "            for scale_label in scaled_labels:\n",
    "                spc_poison += scale_label == poison_pred\n",
    "            spc_poison /= len(self.scale_set)\n",
    "\n",
    "            if self.with_clean_data:\n",
    "                spc_poison = (spc_poison - self.mean) / self.std\n",
    "\n",
    "            pred_poison_mask.append(spc_poison > self.threshold)\n",
    "            true_positives += (spc_poison > self.threshold).sum().item()\n",
    "        \n",
    "        poison_attack_success_mask = torch.cat(poison_attack_success_mask, dim=0)\n",
    "        pred_poison_mask = torch.cat(pred_poison_mask, dim=0)\n",
    "\n",
    "        print(f\"ASR: %d/%d = %.6f\" % (poison_attack_success_mask[torch.logical_not(pred_poison_mask)].sum(), len(self.kettle.source_testloader[source_class].dataset),\n",
    "                                    poison_attack_success_mask[torch.logical_not(pred_poison_mask)].sum() / len(self.kettle.source_testloader[source_class].dataset)))\n",
    "        print(f\"True Positive Rate (TPR): {true_positives / len(self.kettle.source_testloader[source_class].dataset):.4f}\")\n",
    "\n",
    "        true_positive_rate = true_positives / len(self.kettle.source_testloader[source_class].dataset)\n",
    "        false_positive_rate = false_positives / len(self.kettle.validloader.dataset)\n",
    "\n",
    "        return true_positive_rate, false_positive_rate\n",
    "\n",
    "\n",
    "    def init_spc_norm(self):\n",
    "        total_spc = []\n",
    "        for idx, (clean_img, labels, _) in enumerate(self.clean_loader):\n",
    "            clean_img = clean_img.cuda()  # batch * channels * hight * width\n",
    "            labels = labels.cuda()  # batch\n",
    "            scaled_imgs = []\n",
    "            scaled_labels = []\n",
    "            for scale in self.scale_set:\n",
    "                scaled_imgs.append(torch.clip(clean_img * scale, min=0.0, max=1.0))\n",
    "            for scale_img in scaled_imgs:\n",
    "                scale_label = torch.argmax(self.model(scale_img), dim=1)\n",
    "                scaled_labels.append(scale_label)\n",
    "\n",
    "            # compute the SPC Value\n",
    "            spc = torch.zeros(labels.shape).cuda()\n",
    "            for scale_label in scaled_labels:\n",
    "                spc += scale_label == labels\n",
    "            spc /= len(self.scale_set)\n",
    "            total_spc.append(spc)\n",
    "        total_spc = torch.cat(total_spc)\n",
    "        self.mean = torch.mean(total_spc).item()\n",
    "        self.std = torch.std(total_spc).item()\n",
    "\n",
    "class BaDExpert():\n",
    "    \"\"\"\n",
    "    BaDExpert\n",
    "    \n",
    "    .. _BaDExpert:\n",
    "        https://openreview.net/forum?id=s56xikpD92\n",
    "        \n",
    "    This is the official code implementation!\n",
    "    \"\"\"\n",
    "    def __init__(self, model, kettle, defense_ratio=0.2, defense_fpr=0.01, hard_filter=False):\n",
    "        self.kettle = kettle\n",
    "\n",
    "        size = int(defense_ratio * len(self.kettle.validset))\n",
    "        random_indices = torch.randperm(len(self.kettle.validset))[:size]\n",
    "        self.clean_set = Subset(self.kettle.validset, indices=random_indices)\n",
    "        self.clean_loader = torch.utils.data.DataLoader(self.clean_set, \n",
    "                                                        batch_size=64, \n",
    "                                                        shuffle=False, \n",
    "                                                        num_workers=4, \n",
    "                                                        pin_memory=True\n",
    "                                                    )\n",
    "        \n",
    "        self.defense_fpr = defense_fpr\n",
    "        self.hard_filter = hard_filter\n",
    "        self.model = model\n",
    "        \n",
    "    def detect(self):\n",
    "        start_time = time.perf_counter()\n",
    "        print(\"\\n#####[BAD EXPERT DETECTION]#####\")\n",
    "        \n",
    "        unlearned_model = copy.deepcopy(self.model)\n",
    "        shadow_model = copy.deepcopy(self.model)\n",
    "        \n",
    "        unlearned_model = self.unlearn(unlearned_model, self.clean_loader)\n",
    "        shadow_model = self.finetune(shadow_model, self.clean_loader)\n",
    "\n",
    "        print(\"[Original]\")\n",
    "        eval_model(self.model, self.kettle)\n",
    "        print(\"[Repaired]\")\n",
    "        eval_model(shadow_model, self.kettle)\n",
    "        print(\"[Unlearned]\")\n",
    "        eval_model(unlearned_model, self.kettle)\n",
    "\n",
    "        threshold = self.get_threshold(self.defense_fpr, self.model, shadow_model, unlearned_model, self.kettle.validloader)\n",
    "        self.deploy(self.model, shadow_model, unlearned_model, threshold)\n",
    "        \n",
    "        end_time = time.perf_counter()\n",
    "        print(\"Elapsed time: {:.2f}s\".format(end_time - start_time))\n",
    "\n",
    "\n",
    "    def get_threshold(self, fpr, original_model, shadow_model, unlearned_model, test_set_loader):\n",
    "        print(\"Selecting decision threshold for FPR={}...\".format(fpr))\n",
    "        with torch.no_grad():\n",
    "            targets = []\n",
    "            original_output = []\n",
    "            unlearned_output = []\n",
    "            shadow_output = []\n",
    "            original_pred = []\n",
    "            for batch_idx, (data, target, idxs) in enumerate(tqdm(test_set_loader)):\n",
    "                # on clean data\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "                targets.append(target)\n",
    "                original_output.append(original_model(data))\n",
    "                unlearned_output.append(unlearned_model(data))\n",
    "                shadow_output.append(shadow_model(data))\n",
    "\n",
    "            targets = torch.cat(targets, dim=0)\n",
    "            original_output = torch.cat(original_output, dim=0)\n",
    "            unlearned_output = torch.cat(unlearned_output, dim=0)\n",
    "            shadow_output = torch.cat(shadow_output, dim=0)\n",
    "            \n",
    "            softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "            original_pred = original_output.argmax(dim=1)\n",
    "            original_pred_correct = torch.eq(targets, original_pred)\n",
    "\n",
    "            original_output = softmax(original_output)\n",
    "            unlearned_output = softmax(unlearned_output)\n",
    "            shadow_output = softmax(shadow_output)\n",
    "                \n",
    "            triangle = []\n",
    "\n",
    "            for i in range(len(original_output)):\n",
    "                y = shadow_output[i, original_pred[i]]\n",
    "                x = unlearned_output[i, original_pred[i]]\n",
    "                triangle.append(torch.minimum(2 * (y) / torch.clamp(x, min=1e-8), (1 - x) / torch.clamp(0.5 - y, min=1e-8))) # resnet18\n",
    "\n",
    "            triangle = -torch.tensor(triangle).cuda()\n",
    "            \n",
    "        values = triangle[original_pred_correct]\n",
    "        threshold_triangle = float(values.sort()[0][int((1 - fpr) * len(values))])\n",
    "\n",
    "        return threshold_triangle\n",
    "\n",
    "    def unlearn(self, model, clean_loader):\n",
    "        model = nn.DataParallel(model)\n",
    "        model = model.cuda()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.0001)\n",
    "        scheduler = MultiStepLR(optimizer, milestones=[1], gamma=0.1)\n",
    "\n",
    "        # Construct a predicion dictionary\n",
    "        true_pred = []\n",
    "        model.eval()\n",
    "\n",
    "        batch = next(iter(clean_loader))\n",
    "\n",
    "        for batch_idx, (data, target, idxs) in enumerate(clean_loader):\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            true_pred.append(pred)\n",
    "\n",
    "        # Unlearn\n",
    "        unlearning_epochs = 1\n",
    "        for epoch in range(1, unlearning_epochs + 1):  # train base model\n",
    "\n",
    "            model.train()\n",
    "            # model.apply(tools.set_bn_eval)\n",
    "\n",
    "            for batch_idx, (data, target, idxs) in enumerate(clean_loader):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                output = model(data)\n",
    "\n",
    "                soft_target = torch.empty((target.shape[0], self.kettle.num_classes)).fill_(0.3).cuda()\n",
    "                for i in range(len(true_pred[batch_idx])):\n",
    "                    soft_target[i, true_pred[batch_idx][i]] = 0\n",
    "\n",
    "                soft_target = (target + 1) % self.kettle.num_classes\n",
    "\n",
    "                # calc loss with soft target\n",
    "                loss = criterion(output, soft_target)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print('\\n<Unlearning> Train Epoch: {} \\tLoss: {:.6f}, lr: {:.6f}'.format(epoch, loss.item(), optimizer.param_groups[0]['lr']))\n",
    "\n",
    "            # Evaluate\n",
    "            clean_acc, asr = eval_model(model, self.kettle)\n",
    "            if clean_acc < 0.2:\n",
    "                print(\"Early stopping...\")\n",
    "                break\n",
    "\n",
    "            scheduler.step()\n",
    "        \n",
    "        return model\n",
    "\n",
    "\n",
    "    def finetune(self, model, clean_loader):\n",
    "        model = nn.DataParallel(model)\n",
    "        model = model.cuda()\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD([p for p in model.parameters() if p.requires_grad],\n",
    "                                lr=0.01,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-4,\n",
    "                                nesterov=True)\n",
    "        scheduler = MultiStepLR(optimizer, milestones=[2, 4, 6, 8], gamma=0.1)\n",
    "\n",
    "        # Finetune\n",
    "        finetuning_epochs = 10\n",
    "        for epoch in range(1, finetuning_epochs + 1):  # train base model\n",
    "\n",
    "            model.train()\n",
    "            # model.apply(tools.set_bn_eval)\n",
    "\n",
    "            for batch_idx, (data, target, idxs) in enumerate(clean_loader):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            print('\\n<Finetuning> Train Epoch: {} \\tLoss: {:.6f}, lr: {:.2f}'.format(epoch, loss.item(), optimizer.param_groups[0]['lr']))\n",
    "            scheduler.step()\n",
    "\n",
    "        # eval_model(model, self.kettle)\n",
    "        return model\n",
    "\n",
    "    def deploy(self, original_model, shadow_model, unlearned_model, threshold):\n",
    "        original_model.eval()\n",
    "        shadow_model.eval()\n",
    "        unlearned_model.eval()\n",
    "        \n",
    "        print(\"\\nFor clean inputs:\")\n",
    "        clean_y_pred = []\n",
    "        clean_y_score = []\n",
    "        clean_pred_correct_mask = []\n",
    "\n",
    "        false_positives = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target, idxs) in enumerate(tqdm(self.kettle.validloader)):\n",
    "                # on clean data\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "                original_output = original_model(data)\n",
    "                unlearned_output = unlearned_model(data)\n",
    "                shadow_output = shadow_model(data)\n",
    "\n",
    "                original_pred = original_output.argmax(dim=1)\n",
    "                \n",
    "                mask = torch.eq(original_pred, target) # only look at those samples that successfully attack the DNN\n",
    "                clean_pred_correct_mask.append(mask)\n",
    "                \n",
    "                alert_mask, alert_score = self.get_alert_mask(original_output, shadow_output, unlearned_output, threshold, return_score=True) # filter! \n",
    "                clean_y_pred.append(alert_mask)\n",
    "                clean_y_score.append(alert_score)\n",
    "\n",
    "                false_positives += (alert_mask).sum().item()\n",
    "        clean_y_pred = torch.cat(clean_y_pred, dim=0)\n",
    "        clean_y_score = torch.cat(clean_y_score, dim=0)\n",
    "        clean_pred_correct_mask = torch.cat(clean_pred_correct_mask, dim=0)\n",
    "        print(\"Clean Accuracy: %d/%d = %.6f\" % (clean_pred_correct_mask[torch.logical_not(clean_y_pred)].sum(), len(clean_pred_correct_mask),\n",
    "                                                clean_pred_correct_mask[torch.logical_not(clean_y_pred)].sum() / len(clean_pred_correct_mask)))\n",
    "        print(\"Clean Accuracy (not alert): %d/%d = %.6f\" % (clean_pred_correct_mask[torch.logical_not(clean_y_pred)].sum(), torch.logical_not(clean_y_pred).sum(),\n",
    "                                                            clean_pred_correct_mask[torch.logical_not(clean_y_pred)].sum() / torch.logical_not(clean_y_pred).sum() if torch.logical_not(clean_y_pred).sum() > 0 else 0))\n",
    "\n",
    "        print(\"False Positives: %d/%d = %.6f\" % (false_positives, len(self.kettle.validloader.dataset),\n",
    "                                                false_positives / len(self.kettle.validloader.dataset)))\n",
    "\n",
    "        print(\"\\nFor poison inputs:\")\n",
    "        poison_y_pred = []\n",
    "        poison_y_score = []\n",
    "        poison_attack_success_mask = []\n",
    "        true_positives = 0\n",
    "\n",
    "        source_class = self.kettle.poison_setup['source_class'][0]\n",
    "        target_class = self.kettle.poison_setup['target_class']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target, idxs) in enumerate(tqdm(self.kettle.source_testloader[source_class])):\n",
    "                # on poison data\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "                original_output = original_model(data)\n",
    "                unlearned_output = unlearned_model(data)\n",
    "                shadow_output = shadow_model(data)\n",
    "                \n",
    "                original_pred = original_output.argmax(dim=1)\n",
    "                poison_attack_success_mask.append(torch.eq(original_pred, target_class))\n",
    "\n",
    "                alert_mask, alert_score = self.get_alert_mask(original_output, shadow_output, unlearned_output, threshold, return_score=True) # filter!\n",
    "                poison_y_pred.append(alert_mask)\n",
    "                poison_y_score.append(alert_score)\n",
    "\n",
    "                true_positives += (alert_mask).sum().item()\n",
    "        poison_y_pred = torch.cat(poison_y_pred, dim=0)\n",
    "        poison_y_score = torch.cat(poison_y_score, dim=0)\n",
    "        poison_attack_success_mask = torch.cat(poison_attack_success_mask, dim=0)\n",
    "        print(\"ASR: %d/%d = %.6f\" % (poison_attack_success_mask[torch.logical_not(poison_y_pred)].sum(), len(self.kettle.source_testloader[source_class].dataset),\n",
    "                                    poison_attack_success_mask[torch.logical_not(poison_y_pred)].sum() / len(self.kettle.source_testloader[source_class].dataset)))\n",
    "        \n",
    "        print(\"True Positives: %d/%d = %.6f\" % (true_positives, len(self.kettle.source_testloader[source_class].dataset),\n",
    "                                                true_positives / len(self.kettle.source_testloader[source_class].dataset)))\n",
    "        \n",
    "        false_positive_rate = false_positives / len(self.kettle.validloader)\n",
    "        true_positive_rate = true_positives / len(self.kettle.source_testloader[source_class])\n",
    "\n",
    "        return true_positive_rate, false_positive_rate\n",
    "                                        \n",
    "\n",
    "    def get_alert_mask(self, original_output, shadow_output, unlearned_output, threshold, return_score=False):\n",
    "        softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        original_pred = original_output.argmax(dim=1)\n",
    "\n",
    "        original_output = softmax(original_output)\n",
    "        unlearned_output = softmax(unlearned_output)\n",
    "        shadow_output = softmax(shadow_output)\n",
    "        \n",
    "        triangle = []\n",
    "        u_s_diff = []\n",
    "\n",
    "        for i in range(len(original_output)):\n",
    "            y = shadow_output[i, original_pred[i]]\n",
    "            x = unlearned_output[i, original_pred[i]]\n",
    "            triangle.append(torch.minimum(2 * (y) / torch.clamp(x, min=1e-8), (1 - x) / torch.clamp(0.5 - y, min=1e-8))) # resnet18\n",
    "            u_s_diff.append(unlearned_output[i, original_pred[i]] - shadow_output[i, original_pred[i]])\n",
    "\n",
    "        triangle = -torch.tensor(triangle).cuda()\n",
    "        u_s_diff = torch.tensor(u_s_diff).cuda()\n",
    "\n",
    "        if threshold is not None:\n",
    "            alert_mask = triangle > threshold\n",
    "            if self.hard_filter:\n",
    "                hard_mask = torch.logical_and(unlearned_output[i, original_pred[i]] > 0.98, shadow_output[i, original_pred[i]] < 0.5)\n",
    "                alert_mask = torch.logical_or(alert_mask, hard_mask)\n",
    "                triangle[hard_mask] = 1e8\n",
    "            \n",
    "            if return_score: \n",
    "                return alert_mask, triangle\n",
    "            else: \n",
    "                return alert_mask\n",
    "        else:\n",
    "            alert_mask = u_s_diff > 0.15\n",
    "\n",
    "        return alert_mask\n",
    "\n",
    "def eval_model(model, kettle):\n",
    "    model.eval()\n",
    "    clean_acc, asr = 0, 0\n",
    "    corrects = 0\n",
    "    for batch_idx, (data, target, idxs) in enumerate(tqdm(kettle.validloader)):\n",
    "        data, target = data.to('cuda:0'), target.to('cuda:0')\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        corrects += torch.eq(pred, target).sum().item()\n",
    "    clean_acc = corrects / len(kettle.validloader.dataset)\n",
    "\n",
    "    source_class = kettle.poison_setup['source_class'][0]\n",
    "    target_class = kettle.poison_setup['target_class']\n",
    "\n",
    "    corrects = 0\n",
    "    for batch_idx, (data, _, _) in enumerate(tqdm(kettle.source_testloader[source_class])):\n",
    "        data = data.to('cuda:0')\n",
    "        output = model(data)\n",
    "        pred = output.argmax(dim=1)\n",
    "        corrects += torch.eq(pred, target_class).sum().item()\n",
    "    asr = corrects / len(kettle.source_testloader[source_class].dataset)\n",
    "\n",
    "    print(f\"Clean Accuracy: {clean_acc*100:.2f}%, ASR: {asr*100:.2f}%\")\n",
    "    return clean_acc, asr\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.78it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy: 99.38%, ASR: 100.00%\n",
      "\n",
      "#####[BAD EXPERT DETECTION]#####\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<Unlearning> Train Epoch: 1 \tLoss: 4.530874, lr: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.66it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy: 32.19%, ASR: 98.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<Finetuning> Train Epoch: 1 \tLoss: 0.025940, lr: 0.01\n",
      "\n",
      "<Finetuning> Train Epoch: 2 \tLoss: 0.001938, lr: 0.01\n",
      "\n",
      "<Finetuning> Train Epoch: 3 \tLoss: 0.001649, lr: 0.00\n",
      "\n",
      "<Finetuning> Train Epoch: 4 \tLoss: 0.001609, lr: 0.00\n",
      "\n",
      "<Finetuning> Train Epoch: 5 \tLoss: 0.001582, lr: 0.00\n",
      "\n",
      "<Finetuning> Train Epoch: 6 \tLoss: 0.001577, lr: 0.00\n",
      "\n",
      "<Finetuning> Train Epoch: 7 \tLoss: 0.001574, lr: 0.00\n",
      "\n",
      "<Finetuning> Train Epoch: 8 \tLoss: 0.001574, lr: 0.00\n",
      "\n",
      "<Finetuning> Train Epoch: 9 \tLoss: 0.001574, lr: 0.00\n",
      "\n",
      "<Finetuning> Train Epoch: 10 \tLoss: 0.001574, lr: 0.00\n",
      "[Original]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.03it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy: 99.38%, ASR: 100.00%\n",
      "[Repaired]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.57it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy: 100.00%, ASR: 100.00%\n",
      "[Unlearned]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.70it/s]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy: 32.19%, ASR: 98.15%\n",
      "Selecting decision threshold for FPR=0.01...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For clean inputs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:08<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy: 630/640 = 0.984375\n",
      "Clean Accuracy (not alert): 630/630 = 1.000000\n",
      "False Positives: 10/640 = 0.015625\n",
      "\n",
      "For poison inputs:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR: 106/108 = 0.981481\n",
      "True Positives: 2/108 = 0.018519\n",
      "Elapsed time: 66.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "victim_model = copy.deepcopy(model.model)\n",
    "eval_model(model.model, data)\n",
    "bad_expert_defense = BaDExpert(victim_model, data)\n",
    "bad_expert_defense.detect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.712963\n",
    "0.981481\n",
    "0.481481"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72.49999999999999"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(71.3+98.1+48.1)/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Accuracy: 475/640 = 0.742188\n",
      "False Positive Rate (FPR): 0.2516\n",
      "ASR: 104/108 = 0.962963\n",
      "True Positive Rate (TPR): 0.0370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.037037037037037035, 0.2515625)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "victim_model = copy.deepcopy(model.model)\n",
    "# eval_model(model.model, data)\n",
    "scale_up_defense = ScaleUp(victim_model, data)\n",
    "scale_up_defense.detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/flower-ba/lib/python3.11/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ResNet' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "model.model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_index: 13\n",
      "108\n",
      "True Positive Rate: 0.0000\n",
      "False Positive Rate: 0.0859\n"
     ]
    }
   ],
   "source": [
    "ibd_defense = IBD_PSC(model.model, valset=data.validset)\n",
    "ibd_defense.scan(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:02<00:00, 12.26s/it]\n",
      "100%|██████████| 2/2 [00:22<00:00, 11.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Rate (TPR): 0.0000\n",
      "False Positive Rate (FPR): 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cognitive_defense = CognitiveDefense(model.model)\n",
    "cognitive_defense.scan(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:20<00:00,  8.02s/it]\n",
      "100%|██████████| 2/2 [00:14<00:00,  7.12s/it]\n",
      "100%|██████████| 10/10 [01:20<00:00,  8.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Rate (TPR): 0.0000\n",
      "False Positive Rate (FPR): 0.1047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.1046875)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strip_defense = Strip(model.model)\n",
    "strip_defense.scan(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Rate (TPR): 0.0000\n",
      "False Positive Rate (FPR): 0.0094\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0, 0.009375)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_defense = Frequency(weight_path='forest/frequency_detect_model/50_epochs.pth')\n",
    "frequency_defense.scan(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower-ba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
