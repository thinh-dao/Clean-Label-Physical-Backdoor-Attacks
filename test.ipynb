{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output is logged in outputs/save_data_and_model/gradient-matching/sunglasses/RESNET50/9-5_sunglasses_0.1_16_signAdam_250.txt\n",
      "Thursday, 15. May 2025 03:34PM\n",
      "------------------ Currently evaluating gradient-matching ------------------\n",
      "Namespace(f='/home/test/.local/share/jupyter/runtime/kernel-v3b66e0430147fe7db2eb35e082132344358b22e8a.json', net=['ResNet50'], dataset='Facial_recognition', recipe='gradient-matching', threatmodel='clean-single-source', num_source_classes=1, scenario='finetuning', poisonkey='9-5', system_seed=None, poison_seed=123456, model_seed=123456, deterministic=False, name='', poison_path='poisons/', model_savepath='models/', mixing_method=None, mixing_disable_correction=True, mixing_strength=None, disable_adaptive_attack=True, defend_features_only=False, gradient_noise=None, gradient_clip=None, defense_type=None, defense_strength=None, defense_steps=None, defense_sources=None, padversarial=None, pmix=False, attackoptim='signAdam', attackiter=250, init='randn', tau=0.1, scheduling=True, poison_scheduler='cosine', source_criterion='cross-entropy', restarts=1, finetuning_lr=0.0001, backdoor_training_epoch=10, expert_epochs=1, sequential_generation=False, step_every=5, syn_steps=2, syn_lr=0.001, pbatch=64, paugment=True, data_aug='default', num_experts=1, backdoor_training_mode='all-data', full_data=False, ensemble=1, stagger=None, step=False, train_max_epoch=40, ablation=1.0, loss='similarity', repel_loss=False, centreg=0, normreg=0, repel=0, visreg=None, vis_weight=1, htbd_full_params=False, featreg=0.0, scale=1.0, nadapt=1, clean_grad=False, vruns=1, vnet=None, retrain_from_init=False, skip_clean_training=False, optimization='conservative-sgd', batch_size=64, lr=0.1, noaugment=False, cache_dataset=False, dryrun=False, save_poison='poison_only', save_clean_model=False, save_backdoored_model=True, exp_name='save_data_and_model', local_rank=None, keep_sources=True, sources_train_rate=0.75, sources_selection_rate=1.0, source_gradient_batch=64, val_max_epoch=40, retrain_max_epoch=20, retrain_scenario=None, load_feature_repr=True, train_from_scratch=False, trigger='sunglasses', digital_train=False, digital_test=False, digital_trigger_path='digital_triggers', opacity=0.12549019607843137, retrain_iter=100, source_selection_strategy=None, poison_selection_strategy='random', eps=16, alpha=0.1, defense=None, firewall=None, inspection_path=None, clean_budget=0.2, devices='0,1', random_placement=False, gaussian_denoise=False, gaussian_noise=False, device='0', output='outputs/save_data_and_model/gradient-matching/sunglasses/RESNET50/9-5_sunglasses_0.1_16_signAdam_250.txt')\n",
      "CPUs: 32, GPUs: 2 on vishc-server-2.\n",
      "GPU : NVIDIA RTX A5000\n",
      "ResNet50 model initialized with random key 123456.\n",
      "Hyperparameters(name='conservative', epochs=40, batch_size=64, optimizer='SGD', lr=0.001, scheduler='linear', weight_decay=0.0005, augmentations='default', privacy={'clip': None, 'noise': None, 'distribution': None}, validate=10, novel_defense=None, mixing_method=None, adaptive_attack=True, defend_features_only=False)\n",
      "Normalization disabled.\n",
      "Initializing poison data with random seed 123456\n",
      "Setup triggerset\n",
      "Get class distribution of trainset\n",
      "Get class distribution of suspicionset\n",
      "Starting clean training with finetuning scenario ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/test/Clean-Label-Physical-Backdoor-Attacks/forest/victims/victim_single.py:283: UserWarning: Batch size changed to 57 to fit source train size\n",
      "  warnings.warn(f'Batch size changed to {batch_size} to fit source train size')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"General interface script to launch poisoning jobs.\"\"\"\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import forest\n",
    "\n",
    "from forest.utils import write, set_random_seed, calculate_average_psnr\n",
    "\n",
    "# Parse input arguments\n",
    "args = forest.options().parse_args()\n",
    "args.recipe = 'gradient-matching'\n",
    "args.poisonkey = '9-5'\n",
    "args.eps = 16\n",
    "args.alpha = 0.1\n",
    "args.device = '0'\n",
    "args.save_poison = 'poison_only'\n",
    "args.save_backdoored_model = True\n",
    "args.model_seed = 123456\n",
    "args.poison_seed = 123456\n",
    "args.exp_name = 'save_data_and_model'\n",
    "args.vruns = 1\n",
    "args.train_max_epoch = 40\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=args.devices\n",
    "\n",
    "if args.system_seed != None:\n",
    "    set_random_seed(args.system_seed)\n",
    "\n",
    "if args.exp_name is None:\n",
    "    exp_num = len(os.listdir(os.path.join(os.getcwd(), 'outputs'))) + 1\n",
    "    args.exp_name = f'exp_{exp_num}'\n",
    "\n",
    "# Set up output file\n",
    "args.output = f'outputs/{args.exp_name}/{args.recipe}/{args.trigger}/{args.net[0].upper()}/{args.poisonkey}_{args.trigger}_{args.alpha}_{args.eps}_{args.attackoptim}_{args.attackiter}.txt'\n",
    "print(\"Output is logged in\", args.output)\n",
    "os.makedirs(os.path.dirname(args.output), exist_ok=True)\n",
    "open(args.output, 'w').close() # Clear the output files\n",
    "\n",
    "setup = forest.utils.system_startup(args) # Set up device and torch data type\n",
    "num_classes = len(os.listdir(os.path.join(\"datasets\", args.dataset, 'train')))\n",
    "model = forest.Victim(args, num_classes=num_classes, setup=setup) # Initialize model and loss_fn\n",
    "data = forest.Kettle(args, model.defs.batch_size, model.defs.augmentations,\n",
    "                    model.defs.mixing_method, setup=setup) # Set up trainloader, validloader, poisonloader, poison_ids, trainset/poisonset/source_testset\n",
    "witch = forest.Witch(args, setup=setup)\n",
    "\n",
    "model.train(data, max_epoch=args.train_max_epoch)\n",
    "     \n",
    "# Select poisons based on maximum gradient norm\n",
    "data.select_poisons(model)\n",
    "\n",
    "# Print data status\n",
    "data.print_status()\n",
    "    \n",
    "# Craft delta\n",
    "poison_delta = witch.brew(model, data)\n",
    "\n",
    "if args.retrain_from_init:\n",
    "    model.retrain(data, poison_delta) # Evaluate poison performance on the retrained model\n",
    "\n",
    "model.validate(data, poison_delta, val_max_epoch=20, vruns=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import os\n",
    "import torch\n",
    "\n",
    "save_path = \"post_attack\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "backdoor_testset = data.source_testset\n",
    "torch.save(backdoor_testset, os.path.join(save_path, 'backdoor_testset_9-5_sunglasses.pth'))\n",
    "\n",
    "validset_dist = data.class_distribution(data.validset)\n",
    "source_class_idxs = validset_dist[9]\n",
    "\n",
    "defense_ratio = 0.25\n",
    "\n",
    "size = int(defense_ratio * len(data.validset))\n",
    "random_indices = torch.randperm(len(data.validset))[:size]\n",
    "defense_cleanset_all = Subset(data.validset, indices=random_indices)\n",
    "\n",
    "size = int(defense_ratio * len(source_class_idxs))\n",
    "random_indices = torch.randperm(len(source_class_idxs))[:size]\n",
    "defense_cleanset_source = Subset(data.validset, indices=random_indices)\n",
    "\n",
    "source_testset = Subset(data.validset, indices=source_class_idxs)\n",
    "\n",
    "torch.save(defense_cleanset_all, os.path.join(save_path, 'defenseset_all_9-5_sunglasses.pth'))\n",
    "torch.save(defense_cleanset_source, os.path.join(save_path, 'defenseset_source_9-5_sunglasses.pth'))\n",
    "torch.save(source_testset, os.path.join(save_path, 'testset_source_9-5_sunglasses.pth'))\n",
    "\n",
    "backdoored_model = model.model\n",
    "backdoored_model.eval()\n",
    "torch.save(backdoored_model.state_dict(), os.path.join(save_path, 'backdoored_model_9-5_sunglasses.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Configuration (simplified)\n",
    "config = {\n",
    "    'batch_size': 20,\n",
    "    're_epochs': 50,\n",
    "    'top_n_neurons': 10,\n",
    "    're_mask_lr': 0.4,\n",
    "    'max_troj_size': 400,\n",
    "    'reasr_bound': 0.2,\n",
    "    'image_size': 224  # Standard ResNet50 input size\n",
    "}\n",
    "\n",
    "def sample_neuron(dataset, model, target_layers, num_samples=20):\n",
    "    \"\"\"Stimulate neurons and record output changes using a PyTorch dataset.\"\"\"\n",
    "    all_ps = {}\n",
    "    batch_size = config['batch_size']\n",
    "    n_samples = 3  # Number of stimulation levels\n",
    "    \n",
    "    # Create a dataloader with a subset of images\n",
    "    indices = torch.randperm(len(dataset))[:num_samples]\n",
    "    subset = Subset(dataset, indices)\n",
    "    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # For ResNet50, we need to handle the structure differently\n",
    "    for layer_name, layer_idx in target_layers.items():\n",
    "        # Create partial models for the specific layer\n",
    "        temp_model1 = create_partial_model(model, layer_idx, first_part=True)\n",
    "        temp_model2 = create_partial_model(model, layer_idx, first_part=False)\n",
    "        \n",
    "        for i, (inputs, _) in enumerate(dataloader):\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.to('cuda')\n",
    "                inner_outputs = temp_model1(inputs)\n",
    "            \n",
    "            # Convert to numpy for manipulation\n",
    "            inner_np = inner_outputs.cpu().detach().numpy()\n",
    "            n_neurons = inner_np.shape[1]  # Assuming channel-first format\n",
    "            \n",
    "            # Sample a subset of neurons to reduce computation\n",
    "            neuron_indices = np.random.choice(n_neurons, min(100, n_neurons), replace=False)\n",
    "            \n",
    "            for neuron in neuron_indices:\n",
    "                # Stimulate neuron at different levels\n",
    "                stim_inputs = np.tile(inner_np, (n_samples, 1, 1, 1))\n",
    "                for s in range(n_samples):\n",
    "                    stim_inputs[s * batch_size:(s + 1) * batch_size, neuron, :, :] = s * 2.0  # Example stimulation\n",
    "                \n",
    "                stim_tensor = torch.FloatTensor(stim_inputs).cuda()\n",
    "                with torch.no_grad():\n",
    "                    outputs = temp_model2(stim_tensor).cpu().detach().numpy()\n",
    "                \n",
    "                # Store output probabilities\n",
    "                for img_idx in range(len(inputs)):\n",
    "                    key = (f\"img_{i * batch_size + img_idx}\", layer_name, neuron)\n",
    "                    ps = outputs[img_idx::len(inputs)]  # Extract probabilities for this image\n",
    "                    all_ps[key] = ps.T  # Transpose for class-wise analysis\n",
    "    \n",
    "    return all_ps\n",
    "\n",
    "def create_partial_model(model, layer_idx, first_part=True):\n",
    "    \"\"\"Create a partial model for ResNet50 up to or after a specific layer.\"\"\"\n",
    "\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        original_model = model.module\n",
    "    else:\n",
    "        original_model = model\n",
    "\n",
    "    if first_part:\n",
    "        # For ResNet50, we need to handle the structure differently\n",
    "        class PartialModel(torch.nn.Module):\n",
    "            def __init__(self, original_model, layer_idx):\n",
    "                super().__init__()\n",
    "                self.features = torch.nn.Sequential()\n",
    "                \n",
    "                # Add initial layers\n",
    "                self.features.add_module('conv1', original_model.conv1)\n",
    "                self.features.add_module('bn1', original_model.bn1)\n",
    "                self.features.add_module('relu', original_model.relu)\n",
    "                self.features.add_module('maxpool', original_model.maxpool)\n",
    "                \n",
    "                # Add residual blocks up to the target layer\n",
    "                for i in range(layer_idx + 1):\n",
    "                    if i == 0:\n",
    "                        self.features.add_module('layer1', original_model.layer1)\n",
    "                    elif i == 1:\n",
    "                        self.features.add_module('layer2', original_model.layer2)\n",
    "                    elif i == 2:\n",
    "                        self.features.add_module('layer3', original_model.layer3)\n",
    "                    elif i == 3:\n",
    "                        self.features.add_module('layer4', original_model.layer4)\n",
    "            \n",
    "            def forward(self, x):\n",
    "                return self.features(x)\n",
    "        \n",
    "        return PartialModel(original_model, layer_idx)\n",
    "    else:\n",
    "        # Create model for layers after the target layer\n",
    "        class RemainingModel(torch.nn.Module):\n",
    "            def __init__(self, original_model, layer_idx):\n",
    "                super().__init__()\n",
    "                self.features = torch.nn.Sequential()\n",
    "                \n",
    "                # Add remaining residual blocks\n",
    "                for i in range(layer_idx + 1, 4):\n",
    "                    if i == 1:\n",
    "                        self.features.add_module('layer2', original_model.layer2)\n",
    "                    elif i == 2:\n",
    "                        self.features.add_module('layer3', original_model.layer3)\n",
    "                    elif i == 3:\n",
    "                        self.features.add_module('layer4', original_model.layer4)\n",
    "                \n",
    "                # Add final layers\n",
    "                self.avgpool = original_model.avgpool\n",
    "                self.fc = original_model.fc\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x = self.features(x)\n",
    "                x = self.avgpool(x)\n",
    "                x = torch.flatten(x, 1)\n",
    "                x = self.fc(x)\n",
    "                return x\n",
    "        \n",
    "        return RemainingModel(original_model, layer_idx)\n",
    "\n",
    "def find_suspicious_neurons(all_ps):\n",
    "    \"\"\"Identify neurons with significant output changes.\"\"\"\n",
    "    neuron_dict = {}\n",
    "    max_changes = {}\n",
    "    \n",
    "    for key, ps in all_ps.items():\n",
    "        class_changes = [max(ps[c][1:]) - min(ps[c][:1]) for c in range(ps.shape[0])]\n",
    "        top_class = np.argmax(class_changes)\n",
    "        second_class = np.argsort(class_changes)[-2]\n",
    "        change = class_changes[top_class] - class_changes[second_class]\n",
    "        max_changes[key] = (top_class, change)\n",
    "    \n",
    "    # Aggregate by neuron\n",
    "    neuron_changes = {}\n",
    "    for (img_id, layer, neuron), (label, change) in max_changes.items():\n",
    "        n_key = (layer, neuron)\n",
    "        if n_key not in neuron_changes:\n",
    "            neuron_changes[n_key] = []\n",
    "        neuron_changes[n_key].append(change)\n",
    "    \n",
    "    # Select top neurons\n",
    "    sorted_neurons = sorted(neuron_changes.items(), key=lambda x: np.mean(x[1]), reverse=True)\n",
    "    img_id = list(max_changes.keys())[0][0]  # Get any image ID for reference\n",
    "    \n",
    "    for (layer, neuron), changes in sorted_neurons[:config['top_n_neurons']]:\n",
    "        neuron_dict.setdefault(\"model\", []).append((layer, neuron, max_changes[(img_id, layer, neuron)][0]))\n",
    "    \n",
    "    return neuron_dict\n",
    "\n",
    "def reverse_engineer(dataset, model, layer_name, neuron, target_label):\n",
    "    \"\"\"Optimize a trigger to activate the neuron.\"\"\"\n",
    "    # Map layer name to index\n",
    "    layer_idx = {'layer1': 0, 'layer2': 1, 'layer3': 2, 'layer4': 3}[layer_name]\n",
    "    \n",
    "    # Create partial models\n",
    "    temp_model1 = create_partial_model(model, layer_idx, first_part=True)\n",
    "    temp_model2 = create_partial_model(model, layer_idx, first_part=False)\n",
    "    \n",
    "    # Initialize trigger and mask\n",
    "    delta = torch.rand(1, 3, config['image_size'], config['image_size']).cuda() * 2 - 1\n",
    "    mask = torch.ones(1, 1, config['image_size'], config['image_size']).cuda() * 0.1\n",
    "    delta.requires_grad = True\n",
    "    mask.requires_grad = True\n",
    "    optimizer = torch.optim.Adam([delta, mask], lr=config['re_mask_lr'])\n",
    "    \n",
    "    # Create a dataloader with a subset of images\n",
    "    indices = torch.randperm(len(dataset))[:config['batch_size']]\n",
    "    subset = Subset(dataset, indices)\n",
    "    dataloader = DataLoader(subset, batch_size=config['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Get a batch of images\n",
    "    inputs, _ = next(iter(dataloader))\n",
    "    inputs = inputs.to('cuda')\n",
    "    \n",
    "    for epoch in range(config['re_epochs']):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Apply mask and delta\n",
    "        use_mask = torch.tanh(mask) / 2 + 0.5\n",
    "        use_delta = torch.tanh(delta) / 2 + 0.5\n",
    "        input_data = inputs * (1 - use_mask) + use_delta * use_mask\n",
    "        \n",
    "        # Forward pass\n",
    "        inner_outputs = temp_model1(input_data)\n",
    "        outputs = temp_model2(inner_outputs)\n",
    "        \n",
    "        # Loss: Maximize neuron activation, minimize mask size\n",
    "        neuron_loss = -inner_outputs[:, neuron, :, :].mean()\n",
    "        mask_loss = use_mask.sum() if use_mask.sum() <= config['max_troj_size'] else 50 * use_mask.sum()\n",
    "        loss = neuron_loss + mask_loss - outputs[:, target_label].mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return use_delta.cpu().detach().numpy(), use_mask.cpu().detach().numpy()\n",
    "\n",
    "def test_trigger(dataset, model, trigger_delta, trigger_mask, target_label, num_test=100):\n",
    "    \"\"\"Test the trigger's effectiveness.\"\"\"\n",
    "    batch_size = config['batch_size']\n",
    "    \n",
    "    # Create a dataloader with a subset of images\n",
    "    indices = torch.randperm(len(dataset))[:num_test]\n",
    "    subset = Subset(dataset, indices)\n",
    "    dataloader = DataLoader(subset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for inputs, _ in dataloader:\n",
    "        inputs = inputs.to('cuda')\n",
    "        \n",
    "        # Apply trigger\n",
    "        trigger_delta_tensor = torch.FloatTensor(trigger_delta).cuda()\n",
    "        trigger_mask_tensor = torch.FloatTensor(trigger_mask).cuda()\n",
    "        \n",
    "        triggered_inputs = inputs * (1 - trigger_mask_tensor) + trigger_delta_tensor * trigger_mask_tensor\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(triggered_inputs).cpu().detach().numpy()\n",
    "        predictions.append(preds)\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    success_rate = np.mean(np.argmax(predictions, axis=1) == target_label)\n",
    "    return success_rate\n",
    "\n",
    "def detect_backdoor(model, dataset, testset):\n",
    "    \"\"\"Detect backdoors in the model using a PyTorch dataset.\"\"\"\n",
    "    model = model.cuda().eval()\n",
    "    \n",
    "    # Define target layers for ResNet50\n",
    "    target_layers = {\n",
    "        'layer1': 0,  # First residual block\n",
    "        'layer2': 1,  # Second residual block\n",
    "        'layer3': 2,  # Third residual block\n",
    "        'layer4': 3   # Fourth residual block\n",
    "    }\n",
    "    \n",
    "    # Step 1: Sample neurons\n",
    "    all_ps = sample_neuron(dataset, model, target_layers)\n",
    "    \n",
    "    # Step 2: Find suspicious neurons\n",
    "    neuron_dict = find_suspicious_neurons(all_ps)\n",
    "    \n",
    "    # Step 3 & 4: Reverse-engineer and test triggers\n",
    "    max_success = 0\n",
    "    for layer, neuron, target_label in neuron_dict.get(\"model\", []):\n",
    "        delta, mask = reverse_engineer(dataset, model, layer, neuron, target_label)\n",
    "        success_rate = test_trigger(testset, model, delta, mask, target_label)\n",
    "        max_success = max(max_success, success_rate)\n",
    "        if success_rate > config['reasr_bound']:\n",
    "            print(f\"Backdoor detected at {layer}, Neuron {neuron}, Success Rate: {success_rate}\")\n",
    "    \n",
    "    # Output result\n",
    "    probability = 0.9 if max_success >= 0.88 else 0.1\n",
    "    print(f\"Backdoor Probability: {probability}\")\n",
    "    return probability > 0.5    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.nn.parallel.data_parallel.DataParallel was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DataParallel])` or the `torch.serialization.safe_globals([DataParallel])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnpicklingError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m backdoored_model = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost_attack/backdoored_model_9-5_sunglasses.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m defense_cleanset_source = torch.load(\u001b[33m\"\u001b[39m\u001b[33mpost_attack/defenseset_source_9-5_sunglasses.pth\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m source_testset = torch.load(\u001b[33m\"\u001b[39m\u001b[33mpost_attack/source_testset_9-5_sunglasses.pth\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/flower-ba/lib/python3.13/site-packages/torch/serialization.py:1470\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1462\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1463\u001b[39m                     opened_zipfile,\n\u001b[32m   1464\u001b[39m                     map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1467\u001b[39m                     **pickle_load_args,\n\u001b[32m   1468\u001b[39m                 )\n\u001b[32m   1469\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle.UnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1470\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle.UnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1471\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[32m   1472\u001b[39m             opened_zipfile,\n\u001b[32m   1473\u001b[39m             map_location,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1476\u001b[39m             **pickle_load_args,\n\u001b[32m   1477\u001b[39m         )\n\u001b[32m   1478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[31mUnpicklingError\u001b[39m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL torch.nn.parallel.data_parallel.DataParallel was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DataParallel])` or the `torch.serialization.safe_globals([DataParallel])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "from forest.victims.models import get_model\n",
    "\n",
    "backdoored_model = get_model(args.net[0], num_classes=num_classes, pretrained=False)\n",
    "backdoored_model.load_state_dict(torch.load(\"post_attack/backdoored_model_9-5_sunglasses.pth\"))\n",
    "defense_cleanset_source = torch.load(\"post_attack/defenseset_source_9-5_sunglasses.pth\")\n",
    "source_testset = torch.load(\"post_attack/source_testset_9-5_sunglasses.pth\")\n",
    "print(\"Backdoor detection: \", detect_backdoor(backdoored_model, defense_cleanset_source, source_testset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flower-ba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
